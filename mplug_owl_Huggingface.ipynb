{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "0.14.1+cu116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsiao-ching/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'MplugOwlTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from pipeline.mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration\n",
    "from pipeline.mplug_owl.tokenization_mplug_owl import MplugOwlTokenizer\n",
    "from pipeline.mplug_owl.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor\n",
    "\n",
    "model = torch.load('/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/mPLUG_model.pth')\n",
    "pretrained_ckpt = 'MAGAer13/mplug-owl-llama-7b'\n",
    "\n",
    "# model = MplugOwlForConditionalGeneration.from_pretrained(\n",
    "#     pretrained_ckpt,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "image_processor = MplugOwlImageProcessor.from_pretrained(pretrained_ckpt)\n",
    "tokenizer = MplugOwlTokenizer.from_pretrained(pretrained_ckpt)\n",
    "processor = MplugOwlProcessor(image_processor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MplugOwlForConditionalGeneration(\n",
       "  (vision_model): MplugOwlVisionModel(\n",
       "    (embeddings): MplugOwlVisionEmbeddings(\n",
       "      (patch_embed): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (pre_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder): MplugOwlVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): MplugOwlVisionEncoderLayer(\n",
       "          (self_attn): MplugOwlVisionAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MplugOwlMLP(\n",
       "            (activation_fn): QuickGELU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNormFp32((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (abstractor): MplugOwlVisualAbstractorModel(\n",
       "    (encoder): MplugOwlVisualAbstractorEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): MplugOwlVisualAbstractorLayer(\n",
       "          (crossattention): MplugOwlVisualAbstractorAttention(\n",
       "            (attention): MplugOwlVisualAbstractorMultiHeadAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MplugOwlVisualAbstractorCrossOutput(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (norm2): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MplugOwlVisualAbstractorMLP(\n",
       "                (act): SiLU()\n",
       "                (w1): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (w2): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "                (w3): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (ffn_ln): LayerNormFp32((2816,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (normk): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): MplugOwlVisualAbstractorLayer(\n",
       "          (crossattention): MplugOwlVisualAbstractorAttention(\n",
       "            (attention): MplugOwlVisualAbstractorMultiHeadAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MplugOwlVisualAbstractorCrossOutput(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (norm2): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MplugOwlVisualAbstractorMLP(\n",
       "                (act): SiLU()\n",
       "                (w1): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (w2): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "                (w3): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (ffn_ln): LayerNormFp32((2816,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (normk): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): MplugOwlVisualAbstractorLayer(\n",
       "          (crossattention): MplugOwlVisualAbstractorAttention(\n",
       "            (attention): MplugOwlVisualAbstractorMultiHeadAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MplugOwlVisualAbstractorCrossOutput(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (norm2): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MplugOwlVisualAbstractorMLP(\n",
       "                (act): SiLU()\n",
       "                (w1): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (w2): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "                (w3): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (ffn_ln): LayerNormFp32((2816,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (normk): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): MplugOwlVisualAbstractorLayer(\n",
       "          (crossattention): MplugOwlVisualAbstractorAttention(\n",
       "            (attention): MplugOwlVisualAbstractorMultiHeadAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MplugOwlVisualAbstractorCrossOutput(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (norm2): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MplugOwlVisualAbstractorMLP(\n",
       "                (act): SiLU()\n",
       "                (w1): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (w2): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "                (w3): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (ffn_ln): LayerNormFp32((2816,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (normk): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): MplugOwlVisualAbstractorLayer(\n",
       "          (crossattention): MplugOwlVisualAbstractorAttention(\n",
       "            (attention): MplugOwlVisualAbstractorMultiHeadAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MplugOwlVisualAbstractorCrossOutput(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (norm2): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MplugOwlVisualAbstractorMLP(\n",
       "                (act): SiLU()\n",
       "                (w1): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (w2): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "                (w3): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (ffn_ln): LayerNormFp32((2816,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (normk): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): MplugOwlVisualAbstractorLayer(\n",
       "          (crossattention): MplugOwlVisualAbstractorAttention(\n",
       "            (attention): MplugOwlVisualAbstractorMultiHeadAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MplugOwlVisualAbstractorCrossOutput(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (norm2): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MplugOwlVisualAbstractorMLP(\n",
       "                (act): SiLU()\n",
       "                (w1): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (w2): Linear(in_features=2816, out_features=1024, bias=True)\n",
       "                (w3): Linear(in_features=1024, out_features=2816, bias=True)\n",
       "                (ffn_ln): LayerNormFp32((2816,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (normk): LayerNormFp32((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (1): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (2): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (3): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (4): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (5): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (6): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (7): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (8): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (9): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (10): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (11): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (12): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (13): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (14): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (15): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (16): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (17): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (18): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (19): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (20): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (21): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (22): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (23): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (24): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (25): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (26): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (27): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (28): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (29): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (30): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "        (31): LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a human/AI template to organize the context as a multi-turn conversation.\n",
    "# <image> denotes an image placehold.\n",
    "prompts = [\n",
    "'''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "Human: <image>\n",
    "Human: What is or What are the object(s) in the photo? What are the functions this/these object(s) featured? Please list the components that can form this/these object(s) in detail.\n",
    "AI: ''']\n",
    "\n",
    "# The image paths should be placed in the image_list and kept in the same order as in the prompts.\n",
    "# We support urls, local file paths and base64 string. You can custom the pre-process of images by modifying the mplug_owl.modeling_mplug_owl.ImageProcessor\n",
    "#image_list = ['/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/assembly.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object in the photo is a microphone. It has a gray exterior and a circular shape. The microphone has four small dots or legs surrounding it. This object can perform several functions, such as recording audio signals, amplifying voices, capturing sounds for playback in various media formats, and for transmitting audio signals over the internet or other communication channels. The four dots, also known as 'legs' or 'spikes', are designed to enhance stability when placed on a surface, which is crucial for accurate audio recording or playback. Additionally, these legs may serve as an attachment point for a support structure, such as a microphone stand, or be used with other microphone accessories, such as a microphone cable.\n"
     ]
    }
   ],
   "source": [
    "# generate kwargs (the same in transformers) can be passed in the do_generate()\n",
    "generate_kwargs = {\n",
    "    'do_sample': True,\n",
    "    'top_k': 25,\n",
    "    'max_length': 512\n",
    "}\n",
    "from PIL import Image\n",
    "images = [Image.open(_) for _ in image_list]\n",
    "inputs = processor(text=prompts, images=images, return_tensors='pt')\n",
    "inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    res = model.generate(**inputs, **generate_kwargs)\n",
    "sentence = tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_of_mPLUG_Owl(image_list, prompt, generate_kwargs):\n",
    "\n",
    "    # mPLUG-Owl\n",
    "    images = [Image.open(_) for _ in image_list]\n",
    "    inputs = processor(text=prompts, images=images, return_tensors='pt')\n",
    "    inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        res = model.generate(**inputs, **generate_kwargs)\n",
    "    sentence = tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store the text into the file that the corresponding image located\n",
    "    image_str = ' '.join(image_list)\n",
    "    folder = image_str.replace(\"assembly.png\", \"\") + \"mPLUG_Owl.txt\"\n",
    "    \n",
    "    with open(folder, 'w') as file:\n",
    "        file.write(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate kwargs (the same in transformers) can be passed in the do_generate()\n",
    "generate_kwargs = {\n",
    "    'do_sample': True,\n",
    "    'top_k': 25,\n",
    "    'max_length': 512\n",
    "}\n",
    "from PIL import Image\n",
    "\n",
    "test_file = [\"/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/assembly.png\"]\n",
    "pipeline_of_mPLUG_Owl(test_file, \"Please list the possible components that may compose the objects in this image, and provide a detailed description, dividing them into main components and sub-components. The more detailed, the better.\", generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [8:05:01<00:00, 2645.57s/it] \n"
     ]
    }
   ],
   "source": [
    "# Process all assembly files in Dataset\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "assemblies = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset\")\n",
    "\n",
    "for assembly in tqdm(assemblies):\n",
    "        try:\n",
    "                files = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/\"+assembly)\n",
    "                for file in files:\n",
    "                        pipeline_of_mPLUG_Owl([\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/\"+assembly+\"/\"+file + \"/assembly.png\"], \"What is or What are the object(s) in the photo? What are the functions this/these object(s) featured? Please list the components that can form this/these object(s) in detail.\", generate_kwargs)\n",
    "        except: \n",
    "                pass     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:08<01:27,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_0中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:17<01:21,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_1中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:29<01:22, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_2中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:39<01:10, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_3中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:46<00:53,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_4中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [00:53<00:41,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_5中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [00:59<00:30,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_6中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [01:06<00:22,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_7中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [01:13<00:14,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_8中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [01:19<00:07,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_9中的資料夾數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:27<00:00,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_10中的資料夾數量為: 741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of folders in Assembly Dataset_all\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "assemblies = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset\")\n",
    "\n",
    "for assembly in tqdm(assemblies):\n",
    "    # 初始化資料夾計數器\n",
    "    folder_count = 0\n",
    "\n",
    "    # 設定要檢查的資料夾路徑\n",
    "    folder_path = \"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/\"+assembly\n",
    "\n",
    "    # 使用 os.walk() 函式遍歷目錄下的所有檔案和資料夾\n",
    "    for _, dirs, _ in os.walk(folder_path):\n",
    "        # 更新資料夾計數器\n",
    "        folder_count += len(dirs)\n",
    "    print(f\"目錄{assembly}中的資料夾數量為: {folder_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:00<00:00,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_0中的assembly.png數量為: 751\n",
      "目錄Assembly Dataset_1中的assembly.png數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:00<00:00,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_2中的assembly.png數量為: 751\n",
      "目錄Assembly Dataset_3中的assembly.png數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [00:00<00:00, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_4中的assembly.png數量為: 751\n",
      "目錄Assembly Dataset_5中的assembly.png數量為: 751\n",
      "目錄Assembly Dataset_6中的assembly.png數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [00:00<00:00, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_7中的assembly.png數量為: 751\n",
      "目錄Assembly Dataset_8中的assembly.png數量為: 751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:01<00:00,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目錄Assembly Dataset_9中的assembly.png數量為: 751\n",
      "目錄Assembly Dataset_10中的assembly.png數量為: 741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of folders in Assembly Dataset_all\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "assemblies = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset\")\n",
    "\n",
    "for assembly in tqdm(assemblies):\n",
    "    # 初始化資料夾計數器\n",
    "    folder_count = 0\n",
    "    files = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/\"+assembly)\n",
    "\n",
    "    for file in files:\n",
    "        \n",
    "        # 設定要檢查的資料夾路徑\n",
    "        folder_path = \"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/\"+assembly+\"/\"+file\n",
    "\n",
    "        if os.path.exists(os.path.join(folder_path, \"assembly.png\")):\n",
    "            folder_count += 1\n",
    "        \n",
    "    print(f\"目錄{assembly}中的assembly.png數量為: {folder_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/images.png', '/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/cookiemonster.png']\n",
      "/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/assembly.png\n",
      "There is an object in the photo that can be described as either a mic for Skype, an FM recorder, or both. This device serves as the audio input and transmitter for computer-based communication systems like Skype and FM systems. It comes with a cable connected to a PC or Laptop, which allows the user to record audio and send it through a communication medium like Skype or any audio transmission system. There are also a few other objects in the photo, but their specific function is not apparent. It is likely that the additional objects are part of the setup for using the mic or the FM recorder system.\n",
      "/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/images.png, /home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/cookiemonster.png\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = \"/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl\"\n",
    "files = os.listdir(folder_path)\n",
    "image_paths = []\n",
    "for file in files:\n",
    "        if file.endswith(\".png\") and not(file.endswith(\"assembly.png\")):  # 確認檔案是否為.png檔\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            image_paths.append(image_path)\n",
    "        elif file.endswith(\"assembly.png\"):\n",
    "            image = os.path.join(folder_path, file)\n",
    "        elif file.endswith(\"mPLUG_Owl.txt\"):\n",
    "            txt_path = os.path.join(folder_path,file)\n",
    "            f = open(txt_path, 'r')\n",
    "            description = f.read()\n",
    "            f.close()\n",
    "image_answer = \", \".join([image_path for image_path in image_paths])          \n",
    "print(image_paths)\n",
    "print(image)\n",
    "print(description)\n",
    "print(image_answer)\n",
    "print(type(image_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理所有資料夾中的.png檔案並輸出jsonl檔案\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "assembly_path = \"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset\"\n",
    "assemblies = os.listdir(assembly_path)\n",
    "\n",
    "for assembly in tqdm(assemblies):\n",
    "    try:\n",
    "        folder_path = os.path.join(assembly_path, assembly) # \"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_0\"\n",
    "        folders = os.listdir(folder_path)\n",
    "        for folder in folders:\n",
    "            assembly_file = os.path.join(folder_path, folder)\n",
    "            print(assembly_file)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the whole assembly dataset to a .jsonl format\n",
    "<p>Inspired by the OwlEval Dataset, we sort out our assembly dataset which consist of lots of components images and the description generated by mPLUG-Owl into a .jsonl format.</p>\n",
    "For example: \n",
    "<code>{\"image\": \"/path/to/whole image\", \"description\": \"The object in the photo is a black metal rod. ...\", \"answer\": \"/path/to/part 1\", \"/path/to /part 2\",...}</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [01:24<06:49, 45.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_1/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [02:02<05:35, 41.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_2/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [02:45<04:55, 42.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_3/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [07:52<13:47, 137.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_4/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [13:15<16:43, 200.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_5/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [17:42<14:49, 222.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_6/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [18:49<08:38, 172.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_7/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [20:02<04:43, 141.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_8/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [20:30<01:46, 106.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_9/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [25:06<00:00, 136.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_10/Fusion 360 Gallery Dataset Public License.docx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# 定義處理函式\n",
    "def process_folder(folder_path, output_file):\n",
    "    files = os.listdir(folder_path)\n",
    "    image_paths = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\") and not(file.endswith(\"assembly.png\")):\n",
    "            image_path = os.path.join(folder_path, file)\n",
    "            image_paths.append(image_path)\n",
    "        elif file.endswith(\"assembly.png\"):\n",
    "            image = os.path.join(folder_path, file)\n",
    "        elif file.endswith(\"mPLUG_Owl.txt\"):\n",
    "            txt_path = os.path.join(folder_path,file)\n",
    "            f = open(txt_path, 'r')\n",
    "            image_description = f.read()\n",
    "            f.close()\n",
    "    \n",
    "    image_answer = \", \".join([image_path for image_path in image_paths])\n",
    "    \n",
    "    with open(output_file, 'a') as f:\n",
    "        data = {\n",
    "            \"image\": image,  # Whole assembly image\n",
    "            \"description\": image_description,\n",
    "            \"answer\": image_answer # assembly parts\n",
    "        }\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "\n",
    "# 設定輸出檔案路徑\n",
    "output_jsonl = \"/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/train.jsonl\"\n",
    "# 處理所有資料夾中的.png檔案並輸出jsonl檔案\n",
    "assembly_path = \"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset\"\n",
    "assemblies = os.listdir(assembly_path)\n",
    "\n",
    "for assembly in tqdm(assemblies):\n",
    "    try:\n",
    "        folder_path = os.path.join(assembly_path, assembly) # \"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/Assembly Dataset_0\"\n",
    "        folders = os.listdir(folder_path)\n",
    "        for folder in folders:\n",
    "            assembly_file = os.path.join(folder_path, folder)\n",
    "            process_folder(assembly_file, output_jsonl)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all assembly data into jsonl format\n",
    "\n",
    "# Origin training data format:\n",
    "''' {\"image\": [\"xxx.jpg\"], \"text\": \"The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, \n",
    "        and polite answers to the user's questions.\\nHuman: <image>\\nHuman: Where is the man located in the image?\\nAI: The man is located in a small restaurant, \n",
    "        sitting and enjoying his meal.\\nHuman: What type of food is the man eating in the image?\\nAI: The man is eating a burrito in the image.\n",
    "        \\nHuman: What can be seen next to the man in the restaurant?\\nAI: Next to the man in the restaurant, there is a fridge filled with cans of soda.\n",
    "        \\nHuman: How does the man feel as he is about to eat his meal?\\nAI: The man looks happy and is smiling as he is about to take a bite of his meal.\n",
    "        \\nHuman: What unique aspect of the photo makes the scene appear distorted or unusual?\\nAI: The unique aspect of this photo is that it was taken using a fish-eye lens, \n",
    "        which creates a distorted, wide-angle view of the scene, making it appear different from a standard perspective.\", \"task_type\": \"llava_sft\"}\n",
    "''' \n",
    "\n",
    "# Our format:\n",
    "'''{\"image\": [\"xxx.png\"], \"text\": \"The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers \n",
    "        to the user's questions.\\nHuman: <image>\\nHuman: What is or What are the object(s) in the photo? What are the functions this/these object(s) featured? Please \n",
    "        list the components that can form this/these object(s) in detail.\\nAI: <images_emb>\",  \n",
    "        \"task_type\": \"llava_sft\"}\n",
    "'''\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "assemblies = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset\") #./Assembly Dataset_0\n",
    "folder_jsonl = \"/home/hsiao-ching/Desktop/palm-e/mPLUG-Owl/configs/train.yaml\"\n",
    "\n",
    "for assembly in tqdm(assemblies):\n",
    "        try:\n",
    "                files = os.listdir(\"/media/hsiao-ching/ADATA HV320/PaLM-E Experiments/Fusion 360 Assembly Data/Assembly Dataset/\"+assembly)\n",
    "                for file in files:\n",
    "                        \n",
    "        except: \n",
    "                pass     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
